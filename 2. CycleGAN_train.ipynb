{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from dataset import DatasetFromFolder\n",
    "from model import Generator, Discriminator\n",
    "\n",
    "import utils\n",
    "import argparse\n",
    "import os, itertools\n",
    "from logger import Logger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, beta1=0.5, beta2=0.999, crop_size=256, dataset='white2yellow_tiger', decay_epoch=1, fliplr=True, input_size=256, lambdaA=10, lambdaB=10, lrD=0.0002, lrG=0.0002, ndf=64, ngf=32, num_epochs=1, num_resnet=6, resize_scale=286)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#Data Set Parameter\n",
    "parser.add_argument('--dataset', required=False, default='white2yellow_tiger', help='input dataset')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='train batch size')\n",
    "parser.add_argument('--input_size', type=int, default=256, help='input size')\n",
    "parser.add_argument('--resize_scale', type=int, default=286, help='resize scale (0 is false)')\n",
    "parser.add_argument('--crop_size', type=int, default=256, help='crop size (0 is false)')\n",
    "parser.add_argument('--fliplr', type=bool, default=True, help='random fliplr True of False')\n",
    "\n",
    "#Model Parameters \n",
    "parser.add_argument('--ngf', type=int, default=32) # number of generator filters\n",
    "parser.add_argument('--ndf', type=int, default=64) # number of discriminator filters\n",
    "parser.add_argument('--num_resnet', type=int, default=6, help='number of resnet blocks in generator')\n",
    "\n",
    "#Learning Parameters\n",
    "parser.add_argument('--num_epochs', type=int, default=1, help='number of train epochs')\n",
    "parser.add_argument('--decay_epoch', type=int, default=1, help='start decaying learning rate after this number')\n",
    "parser.add_argument('--lrG', type=float, default=0.0002, help='learning rate for generator, default=0.0002')\n",
    "parser.add_argument('--lrD', type=float, default=0.0002, help='learning rate for discriminator, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "parser.add_argument('--lambdaA', type=float, default=10, help='lambdaA for cycle loss')\n",
    "parser.add_argument('--lambdaB', type=float, default=10, help='lambdaB for cycle loss')\n",
    "\n",
    "#parser.add_argument('--cuda', action='store_true', help='use GPU computation')\n",
    "#parser.add_argument('--n_cpu', type=int, default=180, help='number of cpu threads to use during batch generation')\n",
    "\n",
    "params = parser.parse_args([])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for loading data and saving results\n",
    "data_dir = 'data/' + params.dataset + '/'\n",
    "save_dir = params.dataset + '_results/'\n",
    "model_dir = params.dataset + '_model/'\n",
    "\n",
    "# Set the logger\n",
    "D_A_log_dir = save_dir + 'D_A_logs'\n",
    "D_B_log_dir = save_dir + 'D_B_logs'\n",
    "D_A_logger = Logger(D_A_log_dir)\n",
    "D_B_logger = Logger(D_B_log_dir)\n",
    "\n",
    "G_A_log_dir = save_dir + 'G_A_logs'\n",
    "G_B_log_dir = save_dir + 'G_B_logs'\n",
    "\n",
    "G_A_logger = Logger(G_A_log_dir)\n",
    "G_B_logger = Logger(G_B_log_dir)\n",
    "\n",
    "cycle_A_log_dir = save_dir + 'cycle_A_logs'\n",
    "cycle_B_log_dir = save_dir + 'cycle_B_logs'\n",
    "\n",
    "cycle_A_logger = Logger(cycle_A_log_dir)\n",
    "cycle_B_logger = Logger(cycle_B_log_dir)\n",
    "\n",
    "img_log_dir = save_dir + 'img_logs'\n",
    "img_logger = Logger(img_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "### 3.2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\torchgan\\lib\\site-packages\\torchvision\\transforms\\transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Scale((params.input_size,params.input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_A = DatasetFromFolder(data_dir, subfolder='trainA', transform=transform,\n",
    "                                resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "train_data_B = DatasetFromFolder(data_dir, subfolder='trainB', transform=transform,\n",
    "                                resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B, batch_size=params.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.4980,  0.4667,  0.4431,  ..., -0.3569, -0.3412, -0.3255],\n",
      "          [ 0.4980,  0.4745,  0.4510,  ..., -0.3725, -0.3569, -0.3412],\n",
      "          [ 0.5059,  0.4745,  0.4510,  ..., -0.3804, -0.3647, -0.3490],\n",
      "          ...,\n",
      "          [ 0.3804,  0.3725,  0.3725,  ..., -0.2078, -0.1529, -0.1137],\n",
      "          [ 0.3882,  0.3804,  0.3804,  ..., -0.1922, -0.1843, -0.1843],\n",
      "          [ 0.3961,  0.3882,  0.3882,  ..., -0.1843, -0.1922, -0.2078]],\n",
      "\n",
      "         [[ 0.6471,  0.6157,  0.5922,  ..., -0.2863, -0.2784, -0.2706],\n",
      "          [ 0.6471,  0.6235,  0.6000,  ..., -0.3020, -0.2863, -0.2863],\n",
      "          [ 0.6549,  0.6235,  0.6000,  ..., -0.3098, -0.2941, -0.2863],\n",
      "          ...,\n",
      "          [ 0.4275,  0.4196,  0.4196,  ...,  0.0588,  0.1137,  0.1529],\n",
      "          [ 0.4353,  0.4275,  0.4275,  ...,  0.0667,  0.0667,  0.0667],\n",
      "          [ 0.4510,  0.4431,  0.4431,  ...,  0.0667,  0.0510,  0.0353]],\n",
      "\n",
      "         [[ 0.6941,  0.6627,  0.6392,  ..., -0.2314, -0.2157, -0.2157],\n",
      "          [ 0.6941,  0.6706,  0.6471,  ..., -0.2471, -0.2314, -0.2235],\n",
      "          [ 0.7020,  0.6706,  0.6471,  ..., -0.2549, -0.2392, -0.2314],\n",
      "          ...,\n",
      "          [ 0.3961,  0.3882,  0.3882,  ...,  0.2706,  0.3255,  0.3725],\n",
      "          [ 0.4039,  0.3961,  0.3961,  ...,  0.2627,  0.2706,  0.2863],\n",
      "          [ 0.4118,  0.4039,  0.4039,  ...,  0.2627,  0.2471,  0.2549]]]])\n"
     ]
    }
   ],
   "source": [
    "test_data_A = DatasetFromFolder(data_dir, subfolder='testA', transform=transform)\n",
    "\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "test_data_B = DatasetFromFolder(data_dir, subfolder='testB', transform=transform)\n",
    "\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get specific test images\n",
    "test_real_A_data = train_data_A.__getitem__(11).unsqueeze(0) # Convert to 4d tensor (BxNxHxW)\n",
    "test_real_B_data = train_data_B.__getitem__(91).unsqueeze(0)\n",
    "print(test_real_A_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model & Optimizers & Criterions\n",
    "### 4.1 Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Jupyter\\meseong\\CycleGAN\\model.py:115: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.conv.weight, mean, std)\n",
      "D:\\Jupyter\\meseong\\CycleGAN\\model.py:117: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.deconv.weight, mean, std)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "  (conv1): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (conv2): ConvBlock(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (conv3): ConvBlock(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (resnet_blocks): Sequential(\n",
      "    (0): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (5): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (deconv1): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv2): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv3): ConvBlock(\n",
      "    (conv): Conv2d(32, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (bn): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      ")\n",
      "Generator(\n",
      "  (pad): ReflectionPad2d((3, 3, 3, 3))\n",
      "  (conv1): ConvBlock(\n",
      "    (conv): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (conv2): ConvBlock(\n",
      "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (conv3): ConvBlock(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (resnet_blocks): Sequential(\n",
      "    (0): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (5): ResnetBlock(\n",
      "      (resnet_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (deconv1): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv2): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv3): ConvBlock(\n",
      "    (conv): Conv2d(32, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (bn): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (4): ConvBlock(\n",
      "      (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv_blocks): Sequential(\n",
      "    (0): ConvBlock(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (4): ConvBlock(\n",
      "      (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nif torch.cuda.is_available():\\n    device = torch.device(\"cuda:0\")\\n    G_A.to(device)\\n    G_B.to(device)\\n    D_A.to(device)\\n    D_B.to(device)\\n    \\n    return G_A, G_B, D_A, D_B\\n\\nprint(G_A)\\nprint(G_B)\\nprint(D_A)\\nprint(D_B)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_A = Generator(3, params.ngf, 3, params.num_resnet) # input_dim, num_filter, output_dim, num_resnet\n",
    "G_B = Generator(3, params.ngf, 3, params.num_resnet)\n",
    "\n",
    "D_A = Discriminator(3, params.ndf, 1) # input_dim, num_filter, output_dim\n",
    "D_B = Discriminator(3, params.ndf, 1)\n",
    "\n",
    "G_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "print(G_A.cuda())\n",
    "print(G_B.cuda())\n",
    "print(D_A.cuda())\n",
    "print(D_B.cuda())\n",
    "\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    G_A.to(device)\n",
    "    G_B.to(device)\n",
    "    D_A.to(device)\n",
    "    D_B.to(device)\n",
    "    \n",
    "    return G_A, G_B, D_A, D_B\n",
    "\n",
    "print(G_A)\n",
    "print(G_B)\n",
    "print(D_A)\n",
    "print(D_B)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=params.lrG, betas=(params.beta1, params.beta2))\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_Loss = torch.nn.MSELoss().cuda()\n",
    "L1_Loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "# # Training GAN\n",
    "D_A_avg_losses = []\n",
    "D_B_avg_losses = []\n",
    "G_A_avg_losses = []\n",
    "G_B_avg_losses = []\n",
    "cycle_A_avg_losses = []\n",
    "cycle_B_avg_losses = []\n",
    "\n",
    "# Generated image pool\n",
    "num_pool = 50\n",
    "fake_A_pool = utils.ImagePool(num_pool)\n",
    "fake_B_pool = utils.ImagePool(num_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/648], D_A_loss: 0.4543, D_B_loss: 0.6490, G_A_loss: 1.1282, G_B_loss: 0.6268\n",
      "Epoch [1/1], Step [11/648], D_A_loss: 0.2658, D_B_loss: 0.4290, G_A_loss: 0.4867, G_B_loss: 0.3630\n",
      "Epoch [1/1], Step [21/648], D_A_loss: 0.2831, D_B_loss: 0.3067, G_A_loss: 0.3806, G_B_loss: 0.3495\n",
      "Epoch [1/1], Step [31/648], D_A_loss: 0.2413, D_B_loss: 0.2594, G_A_loss: 0.3592, G_B_loss: 0.2811\n",
      "Epoch [1/1], Step [41/648], D_A_loss: 0.5316, D_B_loss: 0.4364, G_A_loss: 0.4711, G_B_loss: 0.6127\n",
      "Epoch [1/1], Step [51/648], D_A_loss: 0.2454, D_B_loss: 0.2203, G_A_loss: 0.2735, G_B_loss: 0.2617\n",
      "Epoch [1/1], Step [61/648], D_A_loss: 0.2688, D_B_loss: 0.3029, G_A_loss: 0.3493, G_B_loss: 0.2305\n",
      "Epoch [1/1], Step [71/648], D_A_loss: 0.2086, D_B_loss: 0.2451, G_A_loss: 0.2954, G_B_loss: 0.2700\n",
      "Epoch [1/1], Step [81/648], D_A_loss: 0.2981, D_B_loss: 0.2193, G_A_loss: 0.3266, G_B_loss: 0.2329\n",
      "Epoch [1/1], Step [91/648], D_A_loss: 0.2356, D_B_loss: 0.4620, G_A_loss: 0.5233, G_B_loss: 0.2139\n",
      "Epoch [1/1], Step [101/648], D_A_loss: 0.2562, D_B_loss: 0.2148, G_A_loss: 0.4019, G_B_loss: 0.2477\n",
      "Epoch [1/1], Step [111/648], D_A_loss: 0.3062, D_B_loss: 0.2870, G_A_loss: 0.2509, G_B_loss: 0.4030\n",
      "Epoch [1/1], Step [121/648], D_A_loss: 0.2104, D_B_loss: 0.1951, G_A_loss: 0.2995, G_B_loss: 0.2150\n",
      "Epoch [1/1], Step [131/648], D_A_loss: 0.1767, D_B_loss: 0.1811, G_A_loss: 0.3073, G_B_loss: 0.1912\n",
      "Epoch [1/1], Step [141/648], D_A_loss: 0.2189, D_B_loss: 0.2249, G_A_loss: 0.2283, G_B_loss: 0.3896\n",
      "Epoch [1/1], Step [151/648], D_A_loss: 0.2460, D_B_loss: 0.2525, G_A_loss: 0.5306, G_B_loss: 0.3435\n",
      "Epoch [1/1], Step [161/648], D_A_loss: 0.2306, D_B_loss: 0.1734, G_A_loss: 0.2599, G_B_loss: 0.3341\n",
      "Epoch [1/1], Step [171/648], D_A_loss: 0.2589, D_B_loss: 0.1277, G_A_loss: 0.4715, G_B_loss: 0.3873\n",
      "Epoch [1/1], Step [181/648], D_A_loss: 0.2795, D_B_loss: 0.2718, G_A_loss: 0.3459, G_B_loss: 0.4572\n",
      "Epoch [1/1], Step [191/648], D_A_loss: 0.1450, D_B_loss: 0.2207, G_A_loss: 0.4947, G_B_loss: 0.2803\n",
      "Epoch [1/1], Step [201/648], D_A_loss: 0.1673, D_B_loss: 0.1622, G_A_loss: 0.3650, G_B_loss: 0.4584\n",
      "Epoch [1/1], Step [211/648], D_A_loss: 0.2024, D_B_loss: 0.1121, G_A_loss: 0.4647, G_B_loss: 0.2642\n",
      "Epoch [1/1], Step [221/648], D_A_loss: 0.2500, D_B_loss: 0.3388, G_A_loss: 0.1257, G_B_loss: 0.4356\n",
      "Epoch [1/1], Step [231/648], D_A_loss: 0.2068, D_B_loss: 0.2173, G_A_loss: 0.2755, G_B_loss: 0.2193\n",
      "Epoch [1/1], Step [241/648], D_A_loss: 0.2446, D_B_loss: 0.1488, G_A_loss: 0.1910, G_B_loss: 0.3987\n",
      "Epoch [1/1], Step [251/648], D_A_loss: 0.2591, D_B_loss: 0.1186, G_A_loss: 0.4644, G_B_loss: 0.3055\n",
      "Epoch [1/1], Step [261/648], D_A_loss: 0.2123, D_B_loss: 0.1303, G_A_loss: 0.2537, G_B_loss: 0.3522\n",
      "Epoch [1/1], Step [271/648], D_A_loss: 0.2154, D_B_loss: 0.1715, G_A_loss: 0.5541, G_B_loss: 0.2917\n",
      "Epoch [1/1], Step [281/648], D_A_loss: 0.2248, D_B_loss: 0.1836, G_A_loss: 0.4967, G_B_loss: 0.3457\n",
      "Epoch [1/1], Step [291/648], D_A_loss: 0.3008, D_B_loss: 0.1392, G_A_loss: 0.2625, G_B_loss: 0.7831\n",
      "Epoch [1/1], Step [301/648], D_A_loss: 0.1751, D_B_loss: 0.3077, G_A_loss: 0.3757, G_B_loss: 0.2088\n",
      "Epoch [1/1], Step [311/648], D_A_loss: 0.1511, D_B_loss: 0.1360, G_A_loss: 0.2889, G_B_loss: 0.2943\n",
      "Epoch [1/1], Step [321/648], D_A_loss: 0.2120, D_B_loss: 0.1770, G_A_loss: 0.5012, G_B_loss: 0.4002\n",
      "Epoch [1/1], Step [331/648], D_A_loss: 0.3315, D_B_loss: 0.1403, G_A_loss: 0.6866, G_B_loss: 0.2362\n",
      "Epoch [1/1], Step [341/648], D_A_loss: 0.2395, D_B_loss: 0.0882, G_A_loss: 0.3375, G_B_loss: 0.4946\n",
      "Epoch [1/1], Step [351/648], D_A_loss: 0.2677, D_B_loss: 0.4403, G_A_loss: 0.0863, G_B_loss: 0.5006\n",
      "Epoch [1/1], Step [361/648], D_A_loss: 0.1795, D_B_loss: 0.2519, G_A_loss: 0.8675, G_B_loss: 0.2660\n",
      "Epoch [1/1], Step [371/648], D_A_loss: 0.1734, D_B_loss: 0.1601, G_A_loss: 0.3231, G_B_loss: 0.3961\n",
      "Epoch [1/1], Step [381/648], D_A_loss: 0.1080, D_B_loss: 0.1172, G_A_loss: 0.2137, G_B_loss: 0.1938\n",
      "Epoch [1/1], Step [391/648], D_A_loss: 0.3052, D_B_loss: 0.2018, G_A_loss: 0.3075, G_B_loss: 0.0665\n",
      "Epoch [1/1], Step [401/648], D_A_loss: 0.2343, D_B_loss: 0.1411, G_A_loss: 0.1688, G_B_loss: 0.2723\n",
      "Epoch [1/1], Step [411/648], D_A_loss: 0.0687, D_B_loss: 0.1102, G_A_loss: 0.5958, G_B_loss: 0.7078\n",
      "Epoch [1/1], Step [421/648], D_A_loss: 0.3318, D_B_loss: 0.2838, G_A_loss: 0.1906, G_B_loss: 0.6404\n",
      "Epoch [1/1], Step [431/648], D_A_loss: 0.5946, D_B_loss: 0.2739, G_A_loss: 0.4553, G_B_loss: 1.2204\n",
      "Epoch [1/1], Step [441/648], D_A_loss: 0.2052, D_B_loss: 0.1267, G_A_loss: 0.4420, G_B_loss: 0.4574\n",
      "Epoch [1/1], Step [451/648], D_A_loss: 0.1453, D_B_loss: 0.2198, G_A_loss: 0.4641, G_B_loss: 0.5342\n",
      "Epoch [1/1], Step [461/648], D_A_loss: 0.2165, D_B_loss: 0.2342, G_A_loss: 0.1626, G_B_loss: 0.7008\n",
      "Epoch [1/1], Step [471/648], D_A_loss: 0.2145, D_B_loss: 0.3692, G_A_loss: 0.1302, G_B_loss: 0.2515\n",
      "Epoch [1/1], Step [481/648], D_A_loss: 0.4372, D_B_loss: 0.1341, G_A_loss: 0.4768, G_B_loss: 0.0445\n",
      "Epoch [1/1], Step [491/648], D_A_loss: 0.2441, D_B_loss: 0.3692, G_A_loss: 0.0905, G_B_loss: 0.2616\n",
      "Epoch [1/1], Step [501/648], D_A_loss: 0.1843, D_B_loss: 0.0762, G_A_loss: 0.7701, G_B_loss: 0.5296\n",
      "Epoch [1/1], Step [511/648], D_A_loss: 0.4337, D_B_loss: 0.1645, G_A_loss: 0.5122, G_B_loss: 0.5229\n",
      "Epoch [1/1], Step [521/648], D_A_loss: 0.1461, D_B_loss: 0.0638, G_A_loss: 0.1485, G_B_loss: 0.3516\n",
      "Epoch [1/1], Step [531/648], D_A_loss: 0.1969, D_B_loss: 0.0967, G_A_loss: 0.2248, G_B_loss: 0.2549\n",
      "Epoch [1/1], Step [541/648], D_A_loss: 0.4851, D_B_loss: 0.1300, G_A_loss: 0.1026, G_B_loss: 0.0885\n",
      "Epoch [1/1], Step [551/648], D_A_loss: 0.2904, D_B_loss: 0.4916, G_A_loss: 0.0422, G_B_loss: 0.5449\n",
      "Epoch [1/1], Step [561/648], D_A_loss: 0.4140, D_B_loss: 0.1750, G_A_loss: 0.3682, G_B_loss: 0.0558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n        # ============ TensorBoard logging ============#\\n        D_A_logger.scalar_summary('losses', D_A_loss.data, step + 1)\\n        D_B_logger.scalar_summary('losses', D_B_loss.data, step + 1)\\n        G_A_logger.scalar_summary('losses', G_A_loss.data, step + 1)\\n        G_B_logger.scalar_summary('losses', G_B_loss.data, step + 1)\\n        cycle_A_logger.scalar_summary('losses', cycle_A_loss.data, step + 1)\\n        cycle_B_logger.scalar_summary('losses', cycle_B_loss.data, step + 1)\\n        step += 1\\n        \\n    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\\n    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\\n    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\\n    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\\n    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\\n    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\\n\\n    # avg loss values for plot\\n    D_A_avg_losses.append(D_A_avg_loss)\\n    D_B_avg_losses.append(D_B_avg_loss)\\n    G_A_avg_losses.append(G_A_avg_loss)\\n    G_B_avg_losses.append(G_B_avg_loss)\\n    cycle_A_avg_losses.append(cycle_A_avg_loss)\\n    cycle_B_avg_losses.append(cycle_B_avg_loss)\\n\\n    # Show result for test image\\n    test_real_A = Variable(test_real_A_data.cuda())\\n    test_fake_B = G_A(test_real_A)\\n    test_recon_A = G_B(test_fake_B)\\n\\n    test_real_B = Variable(test_real_B_data.cuda())\\n    test_fake_A = G_B(test_real_B)\\n    test_recon_B = G_A(test_fake_A)\\n\\n    utils.plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\\n                            epoch, save=True, save_dir=save_dir)\\n\\n    # log the images\\n    result_AtoB = np.concatenate((utils.to_np(test_real_A), utils.to_np(test_fake_B), utils.to_np(test_recon_A)), axis=3)\\n    result_BtoA = np.concatenate((utils.to_np(test_real_B), utils.to_np(test_fake_A), utils.to_np(test_recon_B)), axis=3)\\n\\n    info = { 'result_AtoB': result_AtoB.transpose(0, 2, 3, 1),  # convert to BxHxWxC\\n             'result_BtoA': result_BtoA.transpose(0, 2, 3, 1) }\\n\\n    for tag, images in info.items():\\n        img_logger.image_summary(tag, images, epoch + 1)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(params.num_epochs):\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    G_A_losses = []\n",
    "    G_B_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "    \n",
    "    # Learing rate decay 설정 구간\n",
    "    if(epoch + 1) > params.decay_epoch:\n",
    "        D_A_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        D_B_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        G_optimizer.param_groups[0]['lr'] -= params.lrG / (params.num_epochs - params.decay_epoch)\n",
    "        \n",
    "    \n",
    "    # training 구간\n",
    "    for i, (real_A, real_B) in enumerate(zip(train_data_loader_A, train_data_loader_B)):\n",
    "        \n",
    "        # input image data\n",
    "        real_A = Variable(real_A.cuda())\n",
    "        real_B = Variable(real_B.cuda())\n",
    "        \n",
    "        # -------------------------- train generator G --------------------------\n",
    "        # A --> B\n",
    "        fake_B = G_A(real_A)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        G_A_loss = MSE_Loss(D_B_fake_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # forward cycle loss\n",
    "        recon_A = G_B(fake_B)\n",
    "        cycle_A_loss = L1_Loss(recon_A, real_A) * params.lambdaA\n",
    "        \n",
    "        # B --> A\n",
    "        fake_A = G_B(real_B)\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        G_B_loss = MSE_Loss(D_A_fake_decision, Variable(torch.ones(D_A_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # backward cycle loss\n",
    "        recon_B = G_A(fake_A)\n",
    "        cycle_B_loss = L1_Loss(recon_B, real_B) * params.lambdaB\n",
    "        \n",
    "        # Back propagation\n",
    "        G_loss = G_A_loss + G_B_loss + cycle_A_loss + cycle_B_loss\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # -------------------------- train discriminator D_A --------------------------\n",
    "        D_A_real_decision = D_A(real_A)\n",
    "        D_A_real_loss = MSE_Loss(D_A_real_decision, Variable(torch.ones(D_A_real_decision.size()).cuda()))\n",
    "        \n",
    "        fake_A = fake_A_pool.query(fake_A)\n",
    "        \n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        D_A_fake_loss = MSE_Loss(D_A_fake_decision, Variable(torch.zeros(D_A_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # Back propagation\n",
    "        D_A_loss = (D_A_real_loss + D_A_fake_loss) * 0.5\n",
    "        D_A_optimizer.zero_grad()\n",
    "        D_A_loss.backward()\n",
    "        D_A_optimizer.step()\n",
    "        \n",
    "        # -------------------------- train discriminator D_B --------------------------\n",
    "        D_B_real_decision = D_B(real_B)\n",
    "        D_B_real_loss = MSE_Loss(D_B_real_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "        \n",
    "        fake_B = fake_B_pool.query(fake_B)\n",
    "        \n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        D_B_fake_loss = MSE_Loss(D_B_fake_decision, Variable(torch.zeros(D_B_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # Back propagation\n",
    "        D_B_loss = (D_B_real_loss + D_B_fake_loss) * 0.5\n",
    "        D_B_optimizer.zero_grad()\n",
    "        D_B_loss.backward()\n",
    "        D_B_optimizer.step()\n",
    "        \n",
    "        # ------------------------ Print -----------------------------\n",
    "        # loss values\n",
    "        D_A_losses.append(D_A_loss.data)\n",
    "        D_B_losses.append(D_B_loss.data)\n",
    "        G_A_losses.append(G_A_loss.data)\n",
    "        G_B_losses.append(G_B_loss.data)\n",
    "        cycle_A_losses.append(cycle_A_loss.data)\n",
    "        cycle_B_losses.append(cycle_B_loss.data)\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], D_A_loss: %.4f, D_B_loss: %.4f, G_A_loss: %.4f, G_B_loss: %.4f'\n",
    "                  % (epoch+1, params.num_epochs, i+1, len(train_data_loader_A), D_A_loss.data, D_B_loss.data, G_A_loss.data, G_B_loss.data))\n",
    "\n",
    "\"\"\"\n",
    "        # ============ TensorBoard logging ============#\n",
    "        D_A_logger.scalar_summary('losses', D_A_loss.data, step + 1)\n",
    "        D_B_logger.scalar_summary('losses', D_B_loss.data, step + 1)\n",
    "        G_A_logger.scalar_summary('losses', G_A_loss.data, step + 1)\n",
    "        G_B_logger.scalar_summary('losses', G_B_loss.data, step + 1)\n",
    "        cycle_A_logger.scalar_summary('losses', cycle_A_loss.data, step + 1)\n",
    "        cycle_B_logger.scalar_summary('losses', cycle_B_loss.data, step + 1)\n",
    "        step += 1\n",
    "        \n",
    "    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\n",
    "    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\n",
    "    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\n",
    "    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\n",
    "    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n",
    "    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_A_avg_losses.append(D_A_avg_loss)\n",
    "    D_B_avg_losses.append(D_B_avg_loss)\n",
    "    G_A_avg_losses.append(G_A_avg_loss)\n",
    "    G_B_avg_losses.append(G_B_avg_loss)\n",
    "    cycle_A_avg_losses.append(cycle_A_avg_loss)\n",
    "    cycle_B_avg_losses.append(cycle_B_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "    test_real_A = Variable(test_real_A_data.cuda())\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = Variable(test_real_B_data.cuda())\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "\n",
    "    utils.plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\n",
    "                            epoch, save=True, save_dir=save_dir)\n",
    "\n",
    "    # log the images\n",
    "    result_AtoB = np.concatenate((utils.to_np(test_real_A), utils.to_np(test_fake_B), utils.to_np(test_recon_A)), axis=3)\n",
    "    result_BtoA = np.concatenate((utils.to_np(test_real_B), utils.to_np(test_fake_A), utils.to_np(test_recon_B)), axis=3)\n",
    "\n",
    "    info = { 'result_AtoB': result_AtoB.transpose(0, 2, 3, 1),  # convert to BxHxWxC\n",
    "             'result_BtoA': result_BtoA.transpose(0, 2, 3, 1) }\n",
    "\n",
    "    for tag, images in info.items():\n",
    "        img_logger.image_summary(tag, images, epoch + 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-15bded120b25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mavg_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcycle_A_avg_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mavg_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcycle_B_avg_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Make gif\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Jupyter\\meseong\\CycleGAN\\utils.py\u001b[0m in \u001b[0;36mplot_loss\u001b[1;34m(avg_losses, num_epochs, save, save_dir, show)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'# of Epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\torchgan\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2503\u001b[0m     \"\"\"\n\u001b[0;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[1;32m-> 2505\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\torchgan\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAANgElEQVR4nO3ccYjfd33H8efLxE6mtY7lBEmi7Vi6Gsqg7ug6hFnRjbR/JP8USaC4SmnArQ5mETocKvWvKUMQsmm2iVPQWv1DD4nkD1fpECO50lmalMAtOnNE6Fm7/lO0Znvvj99P77hcct/e/e4u3vv5gMDv+/t9fr9758PdM798f/f7paqQJG1/r9rqASRJm8PgS1ITBl+SmjD4ktSEwZekJgy+JDWxavCTfC7Jc0meucLtSfLpJHNJnk7ytsmPKUlaryHP8D8PHLjK7XcB+8Z/jgL/tP6xJEmTtmrwq+oJ4GdXWXII+EKNnALekORNkxpQkjQZOyfwGLuBC0uO58fX/WT5wiRHGf0vgNe+9rV/dMstt0zgy0tSH08++eRPq2pqLfedRPCzwnUrfl5DVR0HjgNMT0/X7OzsBL68JPWR5L/Xet9J/JbOPLB3yfEe4OIEHleSNEGTCP4M8N7xb+vcAbxYVZedzpEkba1VT+kk+TJwJ7AryTzwUeDVAFX1GeAEcDcwB7wEvG+jhpUkrd2qwa+qI6vcXsBfTWwiSdKG8J22ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNTEo+EkOJDmXZC7Jwyvc/uYkjyd5KsnTSe6e/KiSpPVYNfhJdgDHgLuA/cCRJPuXLfs74LGqug04DPzjpAeVJK3PkGf4twNzVXW+ql4GHgUOLVtTwOvHl28ALk5uREnSJAwJ/m7gwpLj+fF1S30MuDfJPHAC+MBKD5TkaJLZJLMLCwtrGFeStFZDgp8Vrqtlx0eAz1fVHuBu4ItJLnvsqjpeVdNVNT01NfXKp5UkrdmQ4M8De5cc7+HyUzb3A48BVNX3gNcAuyYxoCRpMoYE/zSwL8lNSa5j9KLszLI1PwbeBZDkrYyC7zkbSbqGrBr8qroEPAicBJ5l9Ns4Z5I8kuTgeNlDwANJfgB8Gbivqpaf9pEkbaGdQxZV1QlGL8Yuve4jSy6fBd4+2dEkSZPkO20lqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0MCn6SA0nOJZlL8vAV1rwnydkkZ5J8abJjSpLWa+dqC5LsAI4BfwbMA6eTzFTV2SVr9gF/C7y9ql5I8saNGliStDZDnuHfDsxV1fmqehl4FDi0bM0DwLGqegGgqp6b7JiSpPUaEvzdwIUlx/Pj65a6Gbg5yXeTnEpyYKUHSnI0yWyS2YWFhbVNLElakyHBzwrX1bLjncA+4E7gCPAvSd5w2Z2qjlfVdFVNT01NvdJZJUnrMCT488DeJcd7gIsrrPlGVf2yqn4InGP0D4Ak6RoxJPingX1JbkpyHXAYmFm25uvAOwGS7GJ0iuf8JAeVJK3PqsGvqkvAg8BJ4Fngsao6k+SRJAfHy04Czyc5CzwOfKiqnt+ooSVJr1yqlp+O3xzT09M1Ozu7JV9bkn5TJXmyqqbXcl/faStJTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITg4Kf5ECSc0nmkjx8lXX3JKkk05MbUZI0CasGP8kO4BhwF7AfOJJk/wrrrgf+Gvj+pIeUJK3fkGf4twNzVXW+ql4GHgUOrbDu48AngJ9PcD5J0oQMCf5u4MKS4/nxdb+W5DZgb1V982oPlORoktkkswsLC694WEnS2g0Jfla4rn59Y/Iq4FPAQ6s9UFUdr6rpqpqempoaPqUkad2GBH8e2LvkeA9wccnx9cCtwHeS/Ai4A5jxhVtJurYMCf5pYF+Sm5JcBxwGZn51Y1W9WFW7qurGqroROAUcrKrZDZlYkrQmqwa/qi4BDwIngWeBx6rqTJJHkhzc6AElSZOxc8iiqjoBnFh23UeusPbO9Y8lSZo032krSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWpiUPCTHEhyLslckodXuP2DSc4meTrJt5O8ZfKjSpLWY9XgJ9kBHAPuAvYDR5LsX7bsKWC6qv4Q+BrwiUkPKklanyHP8G8H5qrqfFW9DDwKHFq6oKoer6qXxoengD2THVOStF5Dgr8buLDkeH583ZXcD3xrpRuSHE0ym2R2YWFh+JSSpHUbEvyscF2tuDC5F5gGPrnS7VV1vKqmq2p6ampq+JSSpHXbOWDNPLB3yfEe4OLyRUneDXwYeEdV/WIy40mSJmXIM/zTwL4kNyW5DjgMzCxdkOQ24LPAwap6bvJjSpLWa9XgV9Ul4EHgJPAs8FhVnUnySJKD42WfBF4HfDXJfyaZucLDSZK2yJBTOlTVCeDEsus+suTyuyc8lyRpwnynrSQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0MCn6SA0nOJZlL8vAKt/9Wkq+Mb/9+khsnPagkaX1WDX6SHcAx4C5gP3Akyf5ly+4HXqiq3wc+Bfz9pAeVJK3PkGf4twNzVXW+ql4GHgUOLVtzCPi38eWvAe9KksmNKUlar50D1uwGLiw5ngf++EprqupSkheB3wV+unRRkqPA0fHhL5I8s5aht6FdLNurxtyLRe7FIvdi0R+s9Y5Dgr/SM/Vawxqq6jhwHCDJbFVND/j62557sci9WOReLHIvFiWZXet9h5zSmQf2LjneA1y80pokO4EbgJ+tdShJ0uQNCf5pYF+Sm5JcBxwGZpatmQH+Ynz5HuDfq+qyZ/iSpK2z6imd8Tn5B4GTwA7gc1V1JskjwGxVzQD/CnwxyRyjZ/aHB3zt4+uYe7txLxa5F4vci0XuxaI170V8Ii5JPfhOW0lqwuBLUhMbHnw/lmHRgL34YJKzSZ5O8u0kb9mKOTfDanuxZN09SSrJtv2VvCF7keQ94++NM0m+tNkzbpYBPyNvTvJ4kqfGPyd3b8WcGy3J55I8d6X3KmXk0+N9ejrJ2wY9cFVt2B9GL/L+F/B7wHXAD4D9y9b8JfCZ8eXDwFc2cqat+jNwL94J/Pb48vs778V43fXAE8ApYHqr597C74t9wFPA74yP37jVc2/hXhwH3j++vB/40VbPvUF78afA24BnrnD73cC3GL0H6g7g+0Med6Of4fuxDItW3YuqeryqXhofnmL0noftaMj3BcDHgU8AP9/M4TbZkL14ADhWVS8AVNVzmzzjZhmyFwW8fnz5Bi5/T9C2UFVPcPX3Mh0CvlAjp4A3JHnTao+70cFf6WMZdl9pTVVdAn71sQzbzZC9WOp+Rv+Cb0er7kWS24C9VfXNzRxsCwz5vrgZuDnJd5OcSnJg06bbXEP24mPAvUnmgRPABzZntGvOK+0JMOyjFdZjYh/LsA0M/nsmuReYBt6xoRNtnavuRZJXMfrU1fs2a6AtNOT7Yiej0zp3Mvpf338kubWq/meDZ9tsQ/biCPD5qvqHJH/C6P0/t1bV/238eNeUNXVzo5/h+7EMi4bsBUneDXwYOFhVv9ik2TbbantxPXAr8J0kP2J0jnJmm75wO/Rn5BtV9cuq+iFwjtE/ANvNkL24H3gMoKq+B7yG0QerdTOoJ8ttdPD9WIZFq+7F+DTGZxnFfruep4VV9qKqXqyqXVV1Y1XdyOj1jINVteYPjbqGDfkZ+TqjF/RJsovRKZ7zmzrl5hiyFz8G3gWQ5K2Mgr+wqVNeG2aA945/W+cO4MWq+slqd9rQUzq1cR/L8Btn4F58Engd8NXx69Y/rqqDWzb0Bhm4Fy0M3IuTwJ8nOQv8L/Chqnp+66beGAP34iHgn5P8DaNTGPdtxyeISb7M6BTervHrFR8FXg1QVZ9h9PrF3cAc8BLwvkGPuw33SpK0At9pK0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDXx/4aZaro1YsjCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Plot average losses\n",
    "avg_losses = []\n",
    "avg_losses.append(D_A_avg_losses)\n",
    "avg_losses.append(D_B_avg_losses)\n",
    "avg_losses.append(G_A_avg_losses)\n",
    "avg_losses.append(G_B_avg_losses)\n",
    "avg_losses.append(cycle_A_avg_losses)\n",
    "avg_losses.append(cycle_B_avg_losses)\n",
    "utils.plot_loss(avg_losses, params.num_epochs, save=True, save_dir=save_dir)\n",
    "\n",
    "# Make gif\n",
    "utils.make_gif(params.dataset, params.num_epochs, save_dir=save_dir)\n",
    "\"\"\"\n",
    "\n",
    "# Save trained parameters of model\n",
    "torch.save(G_A.state_dict(), model_dir + 'generator_A_param.pkl')\n",
    "torch.save(G_B.state_dict(), model_dir + 'generator_B_param.pkl')\n",
    "torch.save(D_A.state_dict(), model_dir + 'discriminator_A_param.pkl')\n",
    "torch.save(D_B.state_dict(), model_dir + 'discriminator_B_param.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_torchgan)",
   "language": "python",
   "name": "conda_torchgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
